<!DOCTYPE html> <html lang=""> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Feature Fetch Acceleration | Navin Kumar M </title> <meta name="author" content="Navin Kumar M"> <meta name="description" content="Know more about how I engineered a dramatic 8× speedup in feature fetching at Branch International through distributed and parallel system design and hands-on optimizations."> <meta name="keywords" content="feature fetching, performance optimization, system design, Branch International, 8x speedup, engineering"> <meta property="og:site_name" content="Navin Kumar M"> <meta property="og:type" content="website"> <meta property="og:title" content="Navin Kumar M | Feature Fetch Acceleration"> <meta property="og:url" content="https://cosmic-heart.github.io/post/2025/feature-fetch-optimization/"> <meta property="og:description" content="Know more about how I engineered a dramatic 8× speedup in feature fetching at Branch International through distributed and parallel system design and hands-on optimizations."> <meta property="og:image" content="/assets/img/prof_pic.jpeg"> <meta property="og:locale" content=""> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Feature Fetch Acceleration"> <meta name="twitter:description" content="Know more about how I engineered a dramatic 8× speedup in feature fetching at Branch International through distributed and parallel system design and hands-on optimizations."> <meta name="twitter:image" content="/assets/img/prof_pic.jpeg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Navin Kumar M"
        },
        "url": "https://cosmic-heart.github.io/post/2025/feature-fetch-optimization/",
        "@type": "WebSite",
        "description": "Know more about how I engineered a dramatic 8× speedup in feature fetching at Branch International through distributed and parallel system design and hands-on optimizations.",
        "headline": "Feature Fetch Acceleration",
        
        "sameAs": ["https://github.com/cosmic-heart","https://www.linkedin.com/in/mnk-navin"],
        
        "name": "Navin Kumar M",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&amp;display=swap" rel="stylesheet"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&amp;family=Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpeg?5f8b04941eedba3e168f478e444fa87d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cosmic-heart.github.io/post/2025/feature-fetch-optimization/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <style>.navbar-brand.site-title{font-family:'Lato',sans-serif;font-weight:700;color:inherit}.navbar-brand.site-title.active-home{color:#2698ba!important}@media(max-width:576px){.navbar-collapse.show ~ .navbar-brand,.navbar-collapse.collapsing ~ .navbar-brand{opacity:0;pointer-events:none}.navbar-brand{position:absolute;left:50%;transform:translateX(-50%);text-align:center;z-index:1;max-width:60%;transition:opacity .3s ease}.navbar-toggler{z-index:2;position:relative}.navbar-collapse{position:absolute;left:0;right:0;top:100%;background:var(--global-bg-color,#fff);padding:1rem 0;margin-top:.5rem;border-radius:8px;box-shadow:0 4px 12px rgba(0,0,0,0.15)}.navbar-collapse .dropdown-menu{position:static!important;float:none;background:transparent;border:0;box-shadow:none;padding:0;display:none}.navbar-collapse .dropdown-menu.show{display:block!important}.navbar-collapse .dropdown-item{color:#333!important;padding:.5rem 1rem;text-align:center;font-size:.95rem}.navbar-collapse .dropdown-toggle::after{margin-left:.5rem}.navbar-collapse .dropdown.show .dropdown-menu{display:block!important}.navbar-nav{text-align:center;width:100%}.navbar-nav .nav-item{padding:.5rem 0;border-bottom:1px solid #e8e8e8}.navbar-nav .nav-item:last-child{border-bottom:0}.navbar-nav .nav-link{font-size:1.1rem;padding:.75rem 1rem;display:block;color:#333!important}.navbar-nav .nav-item.active .nav-link{color:#2698ba!important;font-weight:600}.navbar-nav .nav-item{display:flex;justify-content:center;align-items:center}.toggle-container{display:flex!important;justify-content:center!important;align-items:center!important;padding:.5rem 0}#search-toggle{display:flex;justify-content:center;align-items:center;width:100%}#search-toggle .nav-link{text-align:center;width:auto;margin:0 auto}}@media(max-width:576px){html[data-theme="dark"] .navbar-collapse,html.dark .navbar-collapse{background:var(--global-bg-color,#1c1c1d)!important;box-shadow:0 4px 12px rgba(255,255,255,0.1)}html[data-theme="dark"] .navbar-nav .nav-item,html.dark .navbar-nav .nav-item{border-bottom-color:rgba(255,255,255,0.1)}html[data-theme="dark"] .navbar-nav .nav-link,html.dark .navbar-nav .nav-link{color:var(--global-text-color,#fff)!important}html[data-theme="dark"] .navbar-nav .nav-item.active .nav-link,html.dark .navbar-nav .nav-item.active .nav-link{color:#2698ba!important}html[data-theme="dark"] .navbar-collapse .dropdown-item,html.dark .navbar-collapse .dropdown-item{color:var(--global-text-color,#fff)!important}}</style> <script>
    // Fix dropdown behavior on mobile
    (function() {
      function setupMobileDropdowns() {
        if (window.innerWidth <= 576) {
          const dropdownToggles = document.querySelectorAll('.navbar-collapse .dropdown-toggle');
          
          dropdownToggles.forEach(toggle => {
            // Remove Bootstrap's data-toggle to prevent default behavior
            toggle.removeAttribute('data-toggle');
            
            // Remove existing listeners by cloning
            const newToggle = toggle.cloneNode(true);
            toggle.parentNode.replaceChild(newToggle, toggle);
            
            newToggle.addEventListener('click', function(e) {
              e.preventDefault();
              e.stopPropagation();
              e.stopImmediatePropagation();
              
              const parent = this.parentElement;
              const menu = parent.querySelector('.dropdown-menu');
              
              // Close other dropdowns
              document.querySelectorAll('.navbar-collapse .dropdown').forEach(dd => {
                if (dd !== parent) {
                  dd.classList.remove('show');
                  const otherMenu = dd.querySelector('.dropdown-menu');
                  if (otherMenu) otherMenu.classList.remove('show');
                }
              });
              
              // Toggle current dropdown
              parent.classList.toggle('show');
              if (menu) menu.classList.toggle('show');
              
              return false;
            }, true);
          });
        }
      }
      
      // Run immediately and after DOM load
      setupMobileDropdowns();
      document.addEventListener('DOMContentLoaded', setupMobileDropdowns);
    })();
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand site-title " href="/"> Navin Kumar M </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link font-weight-bold" href="/post/index.html">Posts </a> </li> <li class="nav-item "> <a class="nav-link font-weight-bold" href="/project/">Projects </a> </li> <li class="nav-item "> <a class="nav-link font-weight-bold" href="/repo/">Repos </a> </li> <li class="nav-item "> <a class="nav-link font-weight-bold" href="/resume/">Resume </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-size: 2rem; font-weight: 700; font-family: 'Ginto Nord', 'Ginto', 'Inter', 'Segoe UI Variable', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;">Feature Fetch Acceleration</h1> <p class="post-meta"> Created on October 20, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> optimization   <i class="fa-solid fa-hashtag fa-sm"></i> distributed-system   <i class="fa-solid fa-hashtag fa-sm"></i> ray   ·   <i class="fa-solid fa-tag fa-sm"></i> optimization   <i class="fa-solid fa-tag fa-sm"></i> branch-international   <i class="fa-solid fa-tag fa-sm"></i> distributed-system </p> </header> <article class="post-content" style="text-align: justify;"> <div id="markdown-content"> <p>In this post, I’ll deep dive into how I engineered a parallel execution system to fetch features from DynamoDB and Feature Service, and optimized the feature request lifecycle to achieve an 8× speedup.</p> <h3 id="overview">Overview</h3> <p>Fetching Features is a time-consuming part of the model training process. Previously, it took up to 72 hours to fetch features for 1M samples. This was clearly suboptimal and unintuitive that feature creation and aggregation should take 3 days for 1M samples. However, one can argue that there are around 50 feature families per user, each with average of 60 features. This translates to 50M feature family fetches and 3 billion individual features in total. This is a significant amount of data to fetch and it’s might take 3 days of time. But large companies working with petabytes of data can’t afford such long processing times, even when constrained by resources. It is a feeling that this process should not take 3 days of time. This reality pushed us to optimize the feature fetching process to make it dramatically more efficient and faster.</p> <p>Feature Fetch is a long-running process consisting of four major components: <code class="language-plaintext highlighter-rouge">RecordsIterator</code>, <code class="language-plaintext highlighter-rouge">DynamoDB Fetcher</code>, <code class="language-plaintext highlighter-rouge">Feature Service Fetcher</code>, and <code class="language-plaintext highlighter-rouge">File Writer</code>.</p> <ol> <li> <b>RecordsIterator</b>: Iterates over the records for which we need to fetch features and passes them to the DynamoDB Fetcher.</li> <li> <b>DynamoDB Fetcher</b>: Retrieves features from DynamoDB.</li> <li> <b>Feature Service Fetcher</b>: Retrieves features from the Feature Service.</li> <li> <b>File Writer</b>: Writes the fetched features to files in batches.</li> </ol> <p>We identified critical inefficiencies in the old flow (discussed in the next section) and solved them by designing a new distibuted and multi-processing architecture with advanced optimization, accelerating feature fetch by approximately 8x reducing the time for 1M samples from 72 hours to around 10 hours. The new <code class="language-plaintext highlighter-rouge">Fast Feature Fetch</code> system is also significantly more stable and scalable (constrained by resources, not the framework), more efficient, and more cost-effective. Throughout this post, I’ll also mention additional optimizations that could further reduce the time by another 2x.</p> <h3 id="major-inefficiencies-in-old-flow">Major Inefficiencies in Old Flow</h3> <p>In the old flow, the RecordsIterator generates records that are passed to the DynamoDB Fetcher. For feature families with cache hits, these features are immediately passed to the File Writer component. For cache misses, events are routed to the Feature Service Fetcher, which retrieves features from the Feature Service and then passes them to the File Writer. Once the File Writer aggregates features for a batch of users, it writes the complete batch to the file system.</p> <p>The fundamental problem is that <b style="font-weight: 600;">all these components run serially in a single process</b>. The DynamoDB Fetcher and Feature Service Fetcher must wait idle while the File Writer is writing to disk. Similarly, while other components are working, the File Writer sits idle. This sequential execution is clearly inefficient and not scalable. Apart from these issues, there are few more in each category which we will see in the upcoming sections.</p> <h5 id="inefficiencies-in-dynamodb-fetcher">Inefficiencies in DynamoDB Fetcher</h5> <p>There are several critical inefficiencies in the DynamoDB Fetcher:</p> <ol> <li> <p><strong>Incorrectly Assuming DynamoDB Fetcher is I/O Bound</strong></p> <p>We initially set a default batch size of 100, assuming the DynamoDB fetcher was I/O bound. However, our profiling revealed it’s actually <strong>CPU bound</strong>. The time spent creating keys and processing batch requests exceeds the time spent fetching items from DynamoDB itself. Even increasing the batch size to 1000 didn’t improve performance proportionally. Through analysis, we discovered that a DynamoDB fetcher running on a single CPU core won’t see performance improvements beyond a connection pool size of 3 (when using shared connections and async requests). With this optimal pool size of 3, a single processor can handle at most <strong>8,000 requests per second</strong>, note: this is after implementing all possible code optimizations.</p> </li> <li> <p><strong>Creating DynamoDB Client for Each Feature Request</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="nd">@asynccontextmanager</span>
 <span class="k">async</span> <span class="k">def</span> <span class="nf">_create_client</span><span class="p">():</span>
     <span class="k">async</span> <span class="k">with</span> <span class="n">self</span><span class="p">.</span><span class="n">session</span><span class="p">.</span><span class="nf">create_client</span><span class="p">(</span><span class="sh">"</span><span class="s">dynamodb</span><span class="sh">"</span><span class="p">,</span> <span class="p">...)</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
         <span class="k">yield</span> <span class="n">client</span>

 <span class="k">async</span> <span class="k">def</span> <span class="nf">get_feature_per_user</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
     <span class="k">async</span> <span class="k">with</span> <span class="nf">_create_client</span><span class="p">()</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
         <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_item</span><span class="p">(...)</span>
</code></pre></div> </div> <p>In the code above, a new DynamoDB client is created for every single feature request. This is extremely inefficient, the overhead of creating and destroying clients for each request becomes a major bottleneck. The solution is to create the client once and reuse it across all requests. Additionally, we can share a single connection pool across all requests, further reducing overhead.</p> </li> <li> <p><strong>Not Utilizing Single Request Read Capacity</strong></p> <p>DynamoDB allows us to read up to 100 records and 25MB of data per request via <code class="language-plaintext highlighter-rouge">batch_get_item</code>. However, when fetching at the user level, we were only utilizing about less than 50% of this capacity per request. While this might seem like a minor optimization superficially, the reality is that creating requests and parsing responses takes significant CPU time. By batching multiple users’ feature families into a single request (up to the 100-record or 25MB limit), we effectively reduce the number of request/response cycles. This reduces both the number of parsings and the number of in-flight requests. We implemented retry mechanisms to handle edge cases where data exceeds 25MB, though in practice, we’ve never hit these retry cases.</p> </li> </ol> <h5 id="inefficiencies-in-feature-service-fetcher">Inefficiencies in Feature Service Fetcher</h5> <ol> <li> <p><strong>Unnecessary Excessive Feature Requests</strong></p> <p>In the old flow, we split each user’s feature families into groups of 3 per request to the Feature Service. This approach made sense for <strong>inference</strong>, where we want to minimize latency for a single user by parallelizing requests. However, for <strong>batch feature fetching</strong>, the goal is different, we want to optimize throughput for all users in the batch, not minimize individual user latency.</p> <p>Consider the math: With 45 feature families per user split into groups of 3, we send 15 requests per user. For 1,000 users, that’s 15,000 requests. The Feature Service runs a synchronous Flask server where each request blocks a thread. The infrastructure is designed with 36 pods, each with 6 effective CPUs, 20 workers, and 150 threads to handle such a huge throughput. However, CPU utilization often hovered around 40%, most CPU cycles were spent either idle or managing the pool of thousands of concurrent requests rather than doing actual computation. Reducing these number of requests would allow the CPUs to spend more time on actual feature computation rather than request management overhead.</p> </li> <li> <p><strong>Slow or I/O-Bound Feature Requests</strong></p> <p>Some feature families are inherently slow. For example, event features require reading from an Amazon RDS database with around 100 billion rows, making this is extremely slow and completely I/O-bound. Similarly, for Kenya, we process a large volume of messages where regex operations (CPU-bound) take considerable time.</p> <p>Here’s the problem: If we include slow feature families in the same request as fast ones, the entire request takes longer as all feature families in a request is executed sequentially. To make it much worser, our feature service is synchronous nature. So if multiple I/O bound requests are in the same batch, they execute sequentially. This means batching all feature families together (as planned above) would force all slow requests to execute one after another, dramatically increasing the total time to fetch features for the batch.</p> </li> <li> <p><strong>Synchronous Nature of Feature Service</strong></p> <p>The Feature Service uses a synchronous Flask server with synchronous REST APIs. Each incoming request blocks a thread. For a typical user, approximately 30% of processing time is spent waiting for I/O from S3, RDS, and other sources.</p> <p>While converting the Feature Service to async would be relatively straightforward, it wouldn’t solve the fundamental problem. It would just shift the bottleneck to the RDS database, which is already slow. Overwhelming RDS with more I/O requests would cause slowness, timeouts, and IOPS-related issues. Upgrading the database to a larger instance with more IOPS would cost thousands of dollars. Even then, the GP2 storage type we use has a hard limit of 64,000 IOPS, which is insufficient for the throughput we might have. Moving to faster storage (like Provisioned IOPS or io2) would require significant effort and cost. Given these constraints, we had to work with the synchronous nature of the Feature Service.</p> </li> </ol> <h5 id="inefficiencies-in-file-writer">Inefficiencies in File Writer</h5> <ol> <li> <p><strong>Synchronous File Writer</strong></p> <p>The File Writer must write to the local file system and then synchronize to S3 for remote backup. This process takes approximately 2-3 minutes for a chunk of 1,000-5,000 users. In the old single-threaded architecture, this meant all other components (DynamoDB Fetcher and Feature Service Fetcher) sat idle during writes. Even in a new parallel system, synchronous file writing would be a bottleneck if we scale up the DynamoDB fetcher and most records have cache hits, the write operations would struggle to keep up with the fetch rate.</p> </li> </ol> <h3 id="new-architecture">New Architecture</h3> <p>To tackle all the inefficiencies outlined above, we designed a completely new distributed and parallel system architecture (Image 1.1) that is highly scalable and efficient. We call this system <code class="language-plaintext highlighter-rouge">Fast Feature Fetch</code> throughout the rest of this post. For this new architecture, we leveraged <code class="language-plaintext highlighter-rouge">Ray</code> for distributed computing. All the components and queues are implemented as Ray actors, enabling true parallelism and fault tolerance.</p> <div style="width: 100%; max-width: 100%; margin: 0 auto; text-align: center; margin-bottom: 2.5em;"> <img src="/assets/img/posts/2025-10-20-feature-fetch-optimization/architecture.png" alt="Feature Fetcher Architecture" style="width: 100%; max-width: 100%; display: block; margin: 0 auto;"> <div style="font-weight: normal; font-size: 0.9em; margin-bottom: 0.5em;"> Image 1.1: Fast Feature Fetch Architecture </div> </div> <p>From the image, you can see we have four major actors and four queues:</p> <ol> <li> <span style="font-weight: 600;">Main Actor</span>: Orchestrates the entire run spawns worker actors, manages configuration, collects metrics, supervises failures, and coordinates graceful shutdown.</li> <li> <span style="font-weight: 600;">DynamoDB Processor Actor</span>: Listens to the input queue and fetches features from DynamoDB as soon as events are added to the queue.</li> <li> <span style="font-weight: 600;">Feature Service Processor Actor</span>: Listens to the feature service queues and fetches features from the Feature Service as soon as events are added.</li> <li> <span style="font-weight: 600;">File Writer Actor</span>: Buffers and writes outputs, performs intelligent batching, and handles the final data transform.</li> <li> <span style="font-weight: 600;">Queues</span>: <ul> <li> <span style="font-weight: 600;">Input Queue</span>: Where feature fetch requests are placed to be fetched from DynamoDB.</li> <li> <span style="font-weight: 600;">Feature Service Fast Queue</span>: Where all feature families in each event are called in a single batch request (1 event → 1 batch feature request), allowing the features within the request to be computed sequentially but efficiently.</li> <li> <span style="font-weight: 600;">Feature Service Slow Queue</span>: Where each feature family mentioned in an event is fetched in separate requests that execute in parallel.</li> <li> <span style="font-weight: 600;">Writer Queue</span>: Where successfully fetched features are placed to be written to files.</li> </ul> </li> </ol> <p>All the above components are standalone actors that can be scaled horizontally based on our needs. Specifically, based on the data volume we need to fetch and the desired speed. The Main Actor serves as the coordinator, while other actors are workers.</p> <p><strong>Important note on scaling</strong>: Simply increasing the number of actors won’t automatically increase speed. The underlying services (DynamoDB, Feature Service, RDS database) must have adequate provisioning, sufficient pods, IOPS, and capacity, and must not be the limiting factor. Speed increases only when existing actors are already being used at full potential and the backend services can handle additional load.</p> <p>For example:</p> <ul> <li>The <strong>Main Actor</strong> is lightweight, spending most of its time listening to worker actors and monitoring system health. Scaling it has no effect.</li> <li>The <strong>File Writer</strong> is mostly I/O-bound and constrained by disk operations. Since it’s already efficient with 1 CPU, scaling it horizontally doesn’t help.</li> <li>The <strong>DynamoDB Fetcher</strong> is CPU-bound when loaded with requests. Scaling it can increase speed, provided DynamoDB has sufficient read capacity provisioned.</li> </ul> <p>Since all these are Python processes constrained by the GIL (Global Interpreter Lock), each actor can effectively use only 1 CPU core unless using multiprocessing inside the actor. Therefore, if CPU is the limiting factor, increasing the number of actors (and thus the number of processes) will increase throughput.</p> <p><strong>System Flow:</strong></p> <p>The actors work together in a producer-consumer pattern:</p> <ol> <li>The DynamoDB Processor and Feature Service Processor fetch features and place results in the writer queue.</li> <li>The File Writer consumes from the writer queue, buffers results, and writes to disk when the buffer is full.</li> <li>The cache file is updated with completed feature families, enabling resumability and incremental processing.</li> <li>Completed files are synced to remote storage (S3) for durability.</li> </ol> <p><strong>Parallelism at the Feature Family Level</strong></p> <p>To handle things in a truly parallel fashion, we can’t wait until all feature families for a user are completed. Unlike the old feature fetch where the smallest entity was a user, in Fast Feature Fetch, the <strong>smallest entity is a feature family</strong>. This means we can write fetched features as soon as we receive them, rather than waiting for all features for a user to complete. We’ll examine this design in detail when we discuss the Writer Actor.</p> <p><strong>Configuration Management</strong></p> <p>We use a pydantic config (<code class="language-plaintext highlighter-rouge">FeatureFetchConfig</code>) object that tracks how many actors need to be scheduled and stores all actor configurations and queue configurations. This config also serves as metadata to track the overall process.</p> <h5 id="new-feature-fetch-api">New Feature Fetch API</h5> <p>Let’s look at how the new feature fetch system can be used:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="nc">FeatureFetchConfig</span><span class="p">(...)</span> <span class="c1"># Defines the system configuration
</span>
<span class="c1"># This starts the feature fetch Ray system, initializes all actors and queues, and their clients.
# It also starts monitoring and logging.
</span><span class="k">with</span> <span class="nc">FastFeatureFetcher</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">as</span> <span class="n">fetcher</span><span class="p">:</span>
    <span class="n">fetcher</span><span class="p">.</span><span class="nf">warm_up</span><span class="p">()</span>              <span class="c1"># Warms up the system
</span>    <span class="n">fetcher</span><span class="p">.</span><span class="nf">fetch_features</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="c1"># Fetches features for the records
</span></code></pre></div></div> <p><strong>Key points:</strong></p> <ol> <li> <p><strong>Context Manager</strong>: Using the context manager ensures the system is cleaned up properly when execution completes. Since we scale up backend services during execution, we need to descale them before exiting. If you don’t use the context manager, you must manually call <code class="language-plaintext highlighter-rouge">fetcher.shutdown()</code> to gracefully shut down the system.</p> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">warm_up()</code></strong>: This method warms up DynamoDB, Feature Service, and RDS databases by pre-scaling them to handle the expected load.</p> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">fetch_features(records)</code></strong>: This method starts the entire feature fetch system and processes all the provided records.</p> </li> </ol> <h6 id="warm-up-step">Warm-Up Step</h6> <p>This is a crucial step to ensure the system is ready to fetch features at maximum speed. During warm-up, the system performs several key operations:</p> <ol> <li> <p><strong>DynamoDB Provisioning</strong>: Sets optimal read and write capacity for DynamoDB tables based on expected throughput. Internal calculations determine the appropriate values based on the number of records and desired completion time.</p> </li> <li> <p><strong>Feature Service Scaling</strong>: Sends sample requests to the Feature Service to trigger horizontal pod autoscaling (HPA) in the Kubernetes cluster, ensuring sufficient pods are available before the main workload begins.</p> </li> <li> <p><strong>RDS Database Scaling</strong>: Vertically scales RDS databases as needed to handle the increased read load. This scaling is particularly important because RDS must be scaled back down to normal levels once feature fetch completes to avoid unnecessary costs.</p> </li> </ol> <p>It’s critical to scale down resources in case of failures, shutdowns, or interruptions. To handle this, we implemented shutdown handlers that ensure cleanup happens even during unexpected terminations.</p> <h6 id="shutdown-handler">Shutdown Handler</h6> <p>To ensure resources are always cleaned up, we register handlers for various termination scenarios:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cleanup_handler</span><span class="p">(</span><span class="n">signum</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">frame</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handles cleanup during shutdown, scaling down resources gracefully.</span><span class="sh">"""</span>
    <span class="n">main_actor</span><span class="p">.</span><span class="nf">shutdown</span><span class="p">()</span>

<span class="c1"># Register cleanup for normal program exit (e.g., uncaught exceptions)
</span><span class="n">atexit_handler</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="nf">cleanup_handler</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">atexit</span><span class="p">.</span><span class="nf">register</span><span class="p">(</span><span class="n">atexit_handler</span><span class="p">)</span>

<span class="c1"># Register cleanup for interrupt signals
</span><span class="n">signal</span><span class="p">.</span><span class="nf">signal</span><span class="p">(</span><span class="n">signal</span><span class="p">.</span><span class="n">SIGINT</span><span class="p">,</span> <span class="n">cleanup_handler</span><span class="p">)</span>  <span class="c1"># Ctrl+C
</span><span class="n">signal</span><span class="p">.</span><span class="nf">signal</span><span class="p">(</span><span class="n">signal</span><span class="p">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">cleanup_handler</span><span class="p">)</span>  <span class="c1"># TERM signal
</span></code></pre></div></div> <ul> <li> <strong><code class="language-plaintext highlighter-rouge">atexit</code> handler</strong>: Runs cleanup when the program exits due to unexpected errors or normal completion.</li> <li> <strong>Signal handlers</strong>: Run cleanup when the program is interrupted by the user (Ctrl+C, SIGINT) or the system (SIGTERM, often sent by systems).</li> </ul> <h5 id="actor-implementation">Actor Implementation</h5> <p>Let’s examine how these actors are implemented at the code level.</p> <p><strong>Base Actor Class</strong></p> <p>The base actor class is inherited by all actors and is responsible for managing the actor lifecycle and the processing loop:</p> <ol> <li> <p><strong><code class="language-plaintext highlighter-rouge">_processor()</code></strong>: An abstract method that must be implemented by subclasses. This is a long-running process that starts when the Main Actor prompts the worker actor to start and runs until the Main Actor signals it to stop.</p> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">start()</code></strong>: Starts the actor’s processing loop by creating an async task for <code class="language-plaintext highlighter-rouge">_processor()</code>.</p> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">stop()</code></strong>: Stops the actor’s processing loop and performs cleanup.</p> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">ActorState</code></strong>: An enum that tracks the current state of the actor (INITIALIZED, READY, STOPPED, COMPLETED, etc.).</p> </li> </ol> <p><strong>Important</strong>: Users can only start, process, or stop the Main Actor. The entire lifecycle of worker actors is managed by the Main Actor.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseServiceActor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FeatureFetcherConfig</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_id</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_actor_id</span><span class="p">()</span>  <span class="c1"># Gets the unique ID of this actor instance
</span>        <span class="c1"># Relevant initialization code here
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">INITIALIZED</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">_processor</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Abstract method: long-running processing loop implemented by subclasses.</span><span class="sh">"""</span>
        <span class="bp">...</span>

    <span class="nd">@check_state</span><span class="p">(</span><span class="n">ActorState</span><span class="p">.</span><span class="n">INITIALIZED</span><span class="p">)</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Starts the actor</span><span class="sh">'</span><span class="s">s processing loop.</span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_watcher_task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">create_task</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_processor</span><span class="p">())</span>
        <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span>

    <span class="nd">@check_state</span><span class="p">(</span><span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span><span class="p">)</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Stops the actor</span><span class="sh">'</span><span class="s">s processing loop and performs cleanup.</span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">STOPPED</span>
        <span class="c1"># Cleanup code here (close connections, flush buffers, etc.)
</span></code></pre></div></div> <p>Ray Async Actors are actors where communication happens through the event loop, which is intrinsic to the actor itself. A Python class becomes an actor when it is decorated with the <code class="language-plaintext highlighter-rouge">@ray.remote</code> decorator. This makes it an entity that is scalable and fault tolerant. Ray actors become async if at least one of their methods is asynchronous. It is necessary to create a new event loop for long-running tasks; otherwise, actors can become unresponsive if any task takes too long. Generally, the <code class="language-plaintext highlighter-rouge">start</code> method launches this long-running task in the same event loop, as it usually won’t block. This is a good point to note, even though it is not used here.</p> <h5 id="main-actor">Main Actor</h5> <p>The Main Actor orchestrates the entire feature fetch system, managing worker lifecycle, routing events, and monitoring health.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MainActor</span><span class="p">(</span><span class="n">BaseServiceActor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FeatureFetcherConfig</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_start_worker_actors_and_queues</span><span class="p">()</span>  <span class="c1"># Spawn all worker actors and initialize queues
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_processor</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Keeps workers alive until shutdown is requested.</span><span class="sh">"""</span>
        <span class="c1"># Keep workers alive until user requests shutdown
</span>        <span class="k">while</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">!=</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">SHUTDOWN</span><span class="p">:</span>
            <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_shutdown_workers</span><span class="p">()</span>  <span class="c1"># Clean shutdown of all workers
</span>
    <span class="nd">@check_state</span><span class="p">(</span><span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span><span class="p">)</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">_fetch_features</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Main API method called by FastFeatureFetcher to start the feature fetch process.
        Runs three concurrent tasks: progress tracking, feature fetching, and failure monitoring.
        </span><span class="sh">"""</span>
        <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">_progress_stats_calculator</span><span class="p">(),</span>  <span class="c1"># Track progress and detect completion
</span>            <span class="n">self</span><span class="p">.</span><span class="nf">_execute_feature_fetch</span><span class="p">(</span><span class="n">records</span><span class="p">),</span>  <span class="c1"># Process records and enqueue events
</span>            <span class="n">self</span><span class="p">.</span><span class="nf">_monitor_failures</span><span class="p">(),</span>  <span class="c1"># Monitor worker health and handle failures
</span>        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_progress_stats_calculator</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Continuously tracks progress stats. When all work is complete,
        transitions the actor to COMPLETED state.
        </span><span class="sh">"""</span>
        <span class="k">while</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span><span class="p">:</span>
            <span class="n">progress</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_progress</span><span class="p">()</span>  <span class="c1"># Get progress from all worker actors
</span>            <span class="k">if</span> <span class="nf">is_completed</span><span class="p">(</span><span class="n">progress</span><span class="p">):</span>
                <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">COMPLETED</span>
                <span class="k">break</span>
            <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_execute_feature_fetch</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Iterates through records and routes them to appropriate queues.
        Skips cached feature families to avoid redundant computation.
        </span><span class="sh">"""</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">check_cached</span><span class="p">(</span><span class="n">record</span><span class="p">):</span>  <span class="c1"># Skip if already computed and cached
</span>                <span class="k">continue</span>
            <span class="n">relevant_queue</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_determine_queue</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>  <span class="c1"># Route to correct queue
</span>            <span class="k">await</span> <span class="nf">put_ray_queue</span><span class="p">(</span><span class="n">relevant_queue</span><span class="p">,</span> <span class="n">record</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_monitor_failures</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Monitors worker actors for failures. Decides whether to stop the system
        or ignore transient failures based on failure patterns.
        </span><span class="sh">"""</span>
        <span class="k">while</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span><span class="p">:</span>
            <span class="n">failures</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_failures</span><span class="p">()</span>  <span class="c1"># Check all workers for failures
</span>            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">_should_stop</span><span class="p">(</span><span class="n">failures</span><span class="p">):</span>
                <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">FAILED</span>
                <span class="k">break</span>
            <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Code Explanation:</strong></p> <p>The Main Actor uses <strong>concurrent execution</strong> via <code class="language-plaintext highlighter-rouge">asyncio.gather()</code> to run three independent tasks simultaneously:</p> <ol> <li> <p><strong>Progress Tracking</strong> (<code class="language-plaintext highlighter-rouge">_progress_stats_calculator</code>): Continuously polls worker actors to get progress statistics. When all queues are empty and all work is complete and writer signals all records are processed, it transitions the actor to the COMPLETED state, signaling that the feature fetch is done.</p> </li> <li> <strong>Record Processing</strong> (<code class="language-plaintext highlighter-rouge">_execute_feature_fetch</code>): Iterates through input records asynchronously. For each record: <ul> <li>Checks if features are already cached (via cache file with key <code class="language-plaintext highlighter-rouge">(user_id, pitc_timestamp, feature_family_id)</code>) to support resumability</li> <li>If not cached, determines the appropriate queue (DynamoDB input queue, Fast Feature Service queue, or Slow Feature Service queue) based on configuration</li> <li>Enqueues the event for processing</li> </ul> </li> <li> <strong>Failure Monitoring</strong> (<code class="language-plaintext highlighter-rouge">_monitor_failures</code>): Continuously monitors the health of all worker actors and also checks if the there are any problems in the features fetched. If it detects critical failures (e.g., actor crashes, repeated errors), it can decide to stop the entire system gracefully rather than continuing with degraded performance.</li> </ol> <p>This concurrent design allows the Main Actor to coordinate the entire system efficiently without blocking on any single operation.</p> <h5 id="dynamodb-processor-actor">DynamoDB Processor Actor</h5> <p>The DynamoDB Processor Actor is responsible for fetching features from DynamoDB. It listens to the input queue and fetches features as soon as events are added.</p> <p><strong>How We Solved the Inefficiencies:</strong></p> <ol> <li> <p><strong>Reusable Client</strong>: We create the DynamoDB client object only once during actor initialization and reuse it for all requests, eliminating the overhead of creating clients per request.</p> </li> <li> <p><strong>Intelligent Batching</strong>: We batch user requests until we reach the maximum batch size of 100 records. The batching logic is adaptive:</p> <ul> <li>If each user requires 10 feature families, we batch 10 users together (10 users × 10 families = 100 items).</li> <li>If each user requires 80 feature families, we send 1 user per <code class="language-plaintext highlighter-rouge">batch_get_item</code> request (80 items).</li> </ul> </li> </ol> <p>This intelligent batching maximizes DynamoDB’s capacity while minimizing request overhead. These optimizations alone yielded approximately <strong>6× speedup</strong> in the DynamoDB fetcher component.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DynamoDBProcessorActor</span><span class="p">(</span><span class="n">BaseServiceActor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FeatureFetcherConfig</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dynamo_db_client</span> <span class="o">=</span> <span class="nc">DynamoDBClient</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_processor</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span><span class="p">:</span>
            <span class="n">record</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">get_ray_queue</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_queue</span><span class="p">)</span>
            <span class="n">hit</span><span class="p">,</span> <span class="n">miss</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">fetch_features_from_dynamo</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
            <span class="k">await</span> <span class="nf">put_ray_queue</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">writer_queue</span><span class="p">,</span> <span class="n">hit</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">miss</span><span class="p">:</span>
                <span class="n">fs_queue</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_decide_fs_queue</span><span class="p">(</span><span class="n">miss</span><span class="p">)</span>  <span class="c1"># Fast or slow queue
</span>                <span class="k">await</span> <span class="nf">put_ray_queue</span><span class="p">(</span><span class="n">fs_queue</span><span class="p">,</span> <span class="n">miss</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">fetch_features_from_dynamo</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
        <span class="k">return</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">dynamo_db_client</span><span class="p">.</span><span class="nf">batch_get_item</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Code Explanation:</strong></p> <p>The DynamoDB Processor follows a simple but effective loop:</p> <ol> <li> <strong>Dequeue</strong>: Pulls events from the input queue (asynchronously waits if queue is empty).</li> <li> <strong>Fetch</strong>: Queries DynamoDB using <code class="language-plaintext highlighter-rouge">batch_get_item</code> with optimized batching.</li> <li> <strong>Route Results</strong>: <ul> <li> <strong>Cache hits</strong> (features found in DynamoDB) → sent directly to Writer Queue.</li> <li> <strong>Cache misses</strong> (features not in DynamoDB) → sent to Feature Service Queue (fast or slow based on feature family characteristics).</li> </ul> </li> </ol> <p>This design decouples DynamoDB fetching from Feature Service fetching, allowing both to run in parallel across different actors.</p> <h5 id="feature-service-processor-actor">Feature Service Processor Actor</h5> <p>The Feature Service Processor Actor is responsible for fetching features from the Feature Service. It listens to both the fast queue and slow queue, fetching features as soon as events are added. The actor uses a <strong>semaphore design pattern</strong> to control concurrency levels for fast and slow requests independently.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeatureServiceProcessorActor</span><span class="p">(</span><span class="n">BaseServiceActor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FeatureFetcherConfig</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feature_service_client</span> <span class="o">=</span> <span class="nc">FeatureServiceClient</span><span class="p">()</span>

        <span class="c1"># Semaphores control maximum concurrent requests for each queue type
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fast_semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nc">Semaphore</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">max_concurrent_fast_requests</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">slow_semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nc">Semaphore</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">max_concurrent_slow_requests</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_processor</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">fast_task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">create_task</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_process_events</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">feature_service_queue_fast</span><span class="p">,</span> <span class="n">is_slow</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
        <span class="n">slow_task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">create_task</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_process_events</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">feature_service_queue_slow</span><span class="p">,</span> <span class="n">is_slow</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
        <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">fast_task</span><span class="p">,</span> <span class="n">slow_task</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_process_events</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">,</span> <span class="n">is_slow</span><span class="p">):</span>
        <span class="k">while</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span><span class="p">:</span>
            <span class="n">event</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">get_ray_queue</span><span class="p">(</span><span class="n">queue</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">is_slow</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">feature_family</span> <span class="ow">in</span> <span class="n">event</span><span class="p">.</span><span class="n">feature_families</span><span class="p">:</span>
                    <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">slow_semaphore</span><span class="p">.</span><span class="nf">acquire</span><span class="p">()</span> 
                    <span class="n">task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">create_task</span><span class="p">(</span>
                        <span class="n">self</span><span class="p">.</span><span class="nf">fetch_features_from_feature_service</span><span class="p">(</span><span class="n">feature_family</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">task</span><span class="p">.</span><span class="nf">add_done_callback</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">slow_semaphore</span><span class="p">.</span><span class="nf">release</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">fast_semaphore</span><span class="p">.</span><span class="nf">acquire</span><span class="p">()</span>  
                <span class="n">task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">create_task</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="nf">fetch_features_from_feature_service</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">task</span><span class="p">.</span><span class="nf">add_done_callback</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">fast_semaphore</span><span class="p">.</span><span class="nf">release</span><span class="p">())</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">fetch_features_from_feature_service</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">event</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Fetches features from Feature Service via HTTP request.</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">feature_service_client</span><span class="p">.</span><span class="nf">fetch_features</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</code></pre></div></div> <h6 id="semaphore-design-pattern-explanation">Semaphore Design Pattern Explanation</h6> <p>The semaphore pattern is crucial for controlling concurrency and preventing the Feature Service from being overwhelmed:</p> <ol> <li> <p><strong>What is a Semaphore?</strong> A semaphore is a synchronization primitive with a counter. When you acquire a semaphore, the counter decreases; when you release it, the counter increases. If the counter reaches zero, further acquire attempts block until a release occurs.</p> </li> <li> <strong>Why Two Semaphores?</strong> We use separate semaphores for fast and slow queues to control their concurrency independently: <ul> <li> <strong>Fast Semaphore</strong>: Limits concurrent batched requests (e.g., 1000 concurrent requests).</li> <li> <strong>Slow Semaphore</strong>: Limits concurrent individual slow requests (e.g., 200 concurrent requests).</li> </ul> </li> <li> <strong>How It Works:</strong> <ul> <li>When a task wants to make a request, it first acquires the appropriate semaphore (blocks if limit is reached).</li> <li>The task executes and fetches features.</li> <li>When the task completes (success or failure), it releases the semaphore via a callback, allowing another task to proceed.</li> </ul> </li> </ol> <p>This pattern ensures we maximize Feature Service utilization without overloading it, while maintaining separate concurrency budgets for different request types.</p> <h6 id="advantage-of-two-queues">Advantage of Two Queues</h6> <p>Using two separate queues in Fast Feature Fetch optimizes both efficiency and scalability. Fast feature families are processed together in batches so they aren’t delayed by slow ones, while slow feature families (like those requiring complex regex, I/O operations) are each sent as parallel requests to avoid bottlenecks. A single queue would mean slow features stall everything, but splitting all features individually would overwhelm the service. The optimal balance is when for a user a request is split into 1 fast family features and x slow family features, such that both takes equal amount of time to complete. By doing this, we can minimize the number of requests to the Feature Service and also compute features at best possible speed. <strong>Impact:</strong></p> <p>This approach delivered massive improvements:</p> <ul> <li> <strong>4× reduction</strong> in feature fetch time per user</li> <li> <strong>CPU utilization increased to 100%</strong> in Feature Service (previously ~40%)</li> <li> <strong>90% reduction</strong> in total requests to Feature Service</li> <li> <strong>2× reduction</strong> in memory usage (fewer threads needed)</li> </ul> <p>The reduced memory footprint allowed us to switch from memory-optimized to compute-optimized instances, increasing CPU per pod and further reducing costs.</p> <h6 id="sharing-connections-in-feature-service-processor">Sharing Connections in Feature Service Processor</h6> <p>HTTP connection overhead is significant when making thousands of requests. By default, each HTTP request would open a new TCP connection, perform a TLS handshake, make the request, and close the connection. This overhead can be 10-50ms per request.</p> <p><strong>Connection Pooling with aiohttp:</strong></p> <p>We use <code class="language-plaintext highlighter-rouge">aiohttp.TCPConnector</code> to create a connection pool that’s shared across all requests. This means:</p> <ul> <li>Connections are reused across multiple requests</li> <li>DNS lookups are cached</li> <li>TCP and TLS handshakes are amortized across many requests</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a connection pool with limits matching our concurrency
</span><span class="n">connector</span> <span class="o">=</span> <span class="n">aiohttp</span><span class="p">.</span><span class="nc">TCPConnector</span><span class="p">(</span>
    <span class="n">limit</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">max_concurrent_fast_requests</span> <span class="o">+</span> <span class="n">config</span><span class="p">.</span><span class="n">max_concurrent_slow_requests</span><span class="p">,</span>
    <span class="n">ttl_dns_cache</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">TTL_DNS_CACHE</span><span class="p">,</span>  <span class="c1"># Cache DNS for 10 minutes
</span>    <span class="n">keepalive_timeout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">KEEPALIVE_TIMEOUT</span><span class="p">,</span>  <span class="c1"># Keep connections alive for reuse
</span>    <span class="n">force_close</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># Don't force close; allow connection reuse
</span>    <span class="n">enable_cleanup_closed</span><span class="o">=</span><span class="bp">True</span>  <span class="c1"># Clean up closed connections automatically
</span><span class="p">)</span>

<span class="c1"># Create a single session that all requests share
</span><span class="n">session</span> <span class="o">=</span> <span class="n">aiohttp</span><span class="p">.</span><span class="nc">ClientSession</span><span class="p">(</span><span class="n">connector</span><span class="o">=</span><span class="n">connector</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">fetch_features_from_feature_service</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">payload</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Makes an HTTP request using the shared session and connection pool.
    The connection is automatically returned to the pool after use.
    </span><span class="sh">"""</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span> <span class="k">as</span> <span class="n">resp</span><span class="p">:</span>
        <span class="n">response_text</span> <span class="o">=</span> <span class="k">await</span> <span class="n">resp</span><span class="p">.</span><span class="nf">text</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response_text</span>

<span class="c1"># Process events, reusing connections from the pool
</span><span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events_to_process</span><span class="p">:</span>
    <span class="n">task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">create_task</span><span class="p">(</span><span class="nf">fetch_features_from_feature_service</span><span class="p">(</span><span class="n">event</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="n">event</span><span class="p">.</span><span class="n">payload</span><span class="p">))</span>
    <span class="n">task</span><span class="p">.</span><span class="nf">add_done_callback</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="nf">_handle_response</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">result</span><span class="p">()))</span>
</code></pre></div></div> <p><strong>Why This Speeds Things Up:</strong></p> <ol> <li> <p><strong>Connection Reuse</strong>: Instead of ~50ms overhead per request, we pay this cost only once per connection. A single connection can handle hundreds of requests.</p> </li> <li> <p><strong>DNS Caching</strong>: DNS lookups (5-20ms each) are cached, saving time on every request after the first.</p> </li> <li> <p><strong>TCP Keep-Alive</strong>: Connections stay open between requests, eliminating the need for TCP handshakes (3-way handshake = 1-2 RTTs).</p> </li> <li> <p><strong>TLS Session Resumption</strong>: TLS sessions can be resumed, avoiding expensive cryptographic handshakes.</p> </li> </ol> <p>For 50,000 requests, connection pooling can save <strong>41+ minutes</strong> (50ms × 50,000 requests = 2,500 seconds) of pure connection overhead.</p> <h5 id="file-writer-actor">File Writer Actor</h5> <p>The File Writer Actor is responsible for writing fetched features to files. It listens to the writer queue and writes features asynchronously, which is crucial for maintaining high throughput in the system.</p> <p><strong>Challenges with Multiple Writers:</strong></p> <p>If we scale up the File Writer Actor (multiple instances), they could potentially write to the same file simultaneously, causing race conditions and data corruption. Our solution:</p> <ol> <li> <strong>Per-Actor Files</strong>: Each File Writer Actor writes to its own file, identified by <code class="language-plaintext highlighter-rouge">actor_id</code> and a <code class="language-plaintext highlighter-rouge">random_id</code> and file_counter to ensure uniqueness.</li> <li> <strong>Buffering</strong>: Features are buffered in memory and written in batches, reducing the number of I/O operations.</li> <li> <strong>Async I/O</strong>: All file operations are asynchronous, preventing the actor from blocking while waiting for disk I/O.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FileWriterActor</span><span class="p">(</span><span class="n">BaseServiceActor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FeatureFetcherConfig</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># Create unique file path for this actor to avoid race conditions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output_file</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">config</span><span class="p">.</span><span class="n">output_dir</span><span class="si">}</span><span class="s">/features_</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">actor_id</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">uuid</span><span class="p">.</span><span class="nf">uuid4</span><span class="p">()</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">file_counter</span><span class="si">}</span><span class="s">.parquet</span><span class="sh">"</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">buffer_size_limit</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">write_buffer_size</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_processor</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Main processing loop: dequeue features, buffer them, and write when buffer is full.
        </span><span class="sh">"""</span>
        <span class="k">while</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">ActorState</span><span class="p">.</span><span class="n">READY</span><span class="p">:</span>
            <span class="n">event</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">get_ray_queue</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">writer_queue</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">put_to_buffer</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">buffer_is_full</span><span class="p">():</span>
                <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">write_buffer_to_file</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">write_buffer_to_file</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">file_writer</span><span class="p">.</span><span class="nf">write_feature_buffer</span><span class="p">()</span>
        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_cache_file</span><span class="p">()</span>
        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">sync_file_to_remote</span><span class="p">()</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">FileWriterActor</code> class manages writing fetched features to disk efficiently and safely in a distributed, high-throughput system. To avoid file corruption when scaling to multiple actors, each actor writes to its own unique output file, leveraging identifiers like <code class="language-plaintext highlighter-rouge">actor_id</code>, a random UUID, and a file counter for naming. Features are not immediately written; instead, they accumulate in an in-memory buffer until a specified size is reached, at which point they are written out in batches. This minimizes frequent disk I/O. All file operations are asynchronous, ensuring the actor remains highly responsive and does not block on potentially slow disk writes. This design supports smooth parallel processing, safeguards data integrity, and maximizes system throughput.</p> <h3 id="conclusion">Conclusion</h3> <p>Through careful analysis and architectural redesign, we built Fast Feature Fetch, a distributed, parallel system that achieves an <strong>8× speedup</strong> in feature fetching. What previously took 72 hours for 1M samples now completes in approximately 10 hours. It is production-ready and has been processing millions of feature fetches reliably. With additional optimizations (discussed in Future Work), we could potentially achieve another 2× improvement.</p> <h5 id="future-work">Future Work</h5> <p>While Fast Feature Fetch shows strong performance, there are a few more ways to improve it:</p> <p><strong>1. Migration to Compute-Optimized Instances</strong></p> <p>By reducing memory usage through batching and the new request flow, we can now use compute-optimized instances that provide more CPU resources for the same workload. This increases CPU capacity and supports higher throughput.</p> <h4 id="2-cpu-based-autoscaling-for-feature-service">2. CPU-Based Autoscaling for Feature Service</h4> <p>Consistently high CPU utilization now allows the Feature Service to autoscale based on real demand. We can scale up when needed and scale down during idle periods, reducing waste and improving efficiency.</p> <p>These changes make the system faster and more resource efficient.</p> <p>This makes Fast Feature Fetch not just faster, but dramatically more cost-efficient.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Navin Kumar M. Last updated: October 20, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/plotly.js@3.0.1/dist/plotly.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script> <script defer src="/assets/js/plotly-setup.js?5e81fc889064852664784cb29c0d6970" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>